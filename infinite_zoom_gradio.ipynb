{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A25FWIZMfXnX"
      },
      "source": [
        "\n",
        "\n",
        "# Infinite zoom - SD\n",
        "\n",
        "---\n",
        "\n",
        "[![An image](https://img.shields.io/static/v1?label=github&message=repository&color=blue&style=for-the-badge&logo=github&logoColor=white)](https://github.com/v8hid/infinite-zoom-stable-diffusion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBE5yk4uPD2q",
        "outputId": "71aa6c3b-fe54-4611-f417-0249ca4c15e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infinite-zoom-stable-diffusion'...\n",
            "remote: Enumerating objects: 312, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 312 (delta 54), reused 65 (delta 36), pack-reused 217 (from 1)\u001b[K\n",
            "Receiving objects: 100% (312/312), 276.51 MiB | 39.20 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Intializing Environment { display-mode: \"form\" }\n",
        "\n",
        "%pip install gradio > /dev/null 2>&1\n",
        "%pip install -qq transformers scipy ftfy accelerate > /dev/null 2>&1\n",
        "%pip install -qq --upgrade diffusers[torch] > /dev/null 2>&1\n",
        "!git clone https://github.com/v8hid/infinite-zoom-stable-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.extend(['infinite-zoom-stable-diffusion/'])\n",
        "from helpers import *\n",
        "from diffusers import StableDiffusionInpaintPipeline, EulerAncestralDiscreteScheduler\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5MJv55nPPTXY"
      },
      "outputs": [],
      "source": [
        "# @title Define zoom functions { display-mode: \"form\" }\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "inpaint_model_list = [\n",
        "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
        "    \"runwayml/stable-diffusion-inpainting\",\n",
        "    \"parlance/dreamlike-diffusion-1.0-inpainting\",\n",
        "    \"ghunkins/stable-diffusion-liberty-inpainting\",\n",
        "    \"ImNoOne/f222-inpainting-diffusers\"\n",
        "]\n",
        "default_prompt = \"A psychedelic jungle with trees that have glowing, fractal-like patterns, Simon stalenhag poster 1920s style, street level view, hyper futuristic, 8k resolution, hyper realistic\"\n",
        "default_negative_prompt = \"frames, borderline, text, charachter, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\"\n",
        "\n",
        "\n",
        "def zoom(\n",
        "    model_id,\n",
        "    prompts_array,\n",
        "    negative_prompt,\n",
        "    num_outpainting_steps,\n",
        "    guidance_scale,\n",
        "    num_inference_steps,\n",
        "    custom_init_image\n",
        "):\n",
        "    prompts = {}\n",
        "    for x in prompts_array:\n",
        "        try:\n",
        "            key = int(x[0])\n",
        "            value = str(x[1])\n",
        "            prompts[key] = value\n",
        "        except ValueError:\n",
        "            pass\n",
        "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "        pipe.scheduler.config)\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "\n",
        "    pipe.safety_checker = None\n",
        "    pipe.enable_attention_slicing()\n",
        "    g_cuda = torch.Generator(device='cuda')\n",
        "\n",
        "    height = 512\n",
        "    width = height\n",
        "\n",
        "    current_image = Image.new(mode=\"RGBA\", size=(height, width))\n",
        "    mask_image = np.array(current_image)[:, :, 3]\n",
        "    mask_image = Image.fromarray(255-mask_image).convert(\"RGB\")\n",
        "    current_image = current_image.convert(\"RGB\")\n",
        "    if (custom_init_image):\n",
        "        current_image = custom_init_image.resize(\n",
        "            (width, height), resample=Image.LANCZOS)\n",
        "    else:\n",
        "        init_images = pipe(prompt=prompts[min(k for k in prompts.keys() if k >= 0)],\n",
        "                           negative_prompt=negative_prompt,\n",
        "                           image=current_image,\n",
        "                           guidance_scale=guidance_scale,\n",
        "                           height=height,\n",
        "                           width=width,\n",
        "                           mask_image=mask_image,\n",
        "                           num_inference_steps=num_inference_steps)[0]\n",
        "        current_image = init_images[0]\n",
        "    mask_width = 128\n",
        "    num_interpol_frames = 30\n",
        "\n",
        "    all_frames = []\n",
        "    all_frames.append(current_image)\n",
        "\n",
        "    for i in range(num_outpainting_steps):\n",
        "        print('Outpaint step: ' + str(i+1) +\n",
        "              ' / ' + str(num_outpainting_steps))\n",
        "\n",
        "        prev_image_fix = current_image\n",
        "\n",
        "        prev_image = shrink_and_paste_on_blank(current_image, mask_width)\n",
        "\n",
        "        current_image = prev_image\n",
        "\n",
        "        # create mask (black image with white mask_width width edges)\n",
        "        mask_image = np.array(current_image)[:, :, 3]\n",
        "        mask_image = Image.fromarray(255-mask_image).convert(\"RGB\")\n",
        "\n",
        "        # inpainting step\n",
        "        current_image = current_image.convert(\"RGB\")\n",
        "        images = pipe(prompt=prompts[max(k for k in prompts.keys() if k <= i)],\n",
        "                      negative_prompt=negative_prompt,\n",
        "                      image=current_image,\n",
        "                      guidance_scale=guidance_scale,\n",
        "                      height=height,\n",
        "                      width=width,\n",
        "                      # generator = g_cuda.manual_seed(seed),\n",
        "                      mask_image=mask_image,\n",
        "                      num_inference_steps=num_inference_steps)[0]\n",
        "        current_image = images[0]\n",
        "        current_image.paste(prev_image, mask=prev_image)\n",
        "\n",
        "        # interpolation steps bewteen 2 inpainted images (=sequential zoom and crop)\n",
        "        for j in range(num_interpol_frames - 1):\n",
        "            interpol_image = current_image\n",
        "            interpol_width = round(\n",
        "                (1 - (1-2*mask_width/height)**(1-(j+1)/num_interpol_frames))*height/2\n",
        "            )\n",
        "            interpol_image = interpol_image.crop((interpol_width,\n",
        "                                                  interpol_width,\n",
        "                                                  width - interpol_width,\n",
        "                                                  height - interpol_width))\n",
        "\n",
        "            interpol_image = interpol_image.resize((height, width))\n",
        "\n",
        "            # paste the higher resolution previous image in the middle to avoid drop in quality caused by zooming\n",
        "            interpol_width2 = round(\n",
        "                (1 - (height-2*mask_width) / (height-2*interpol_width)) / 2*height\n",
        "            )\n",
        "            prev_image_fix_crop = shrink_and_paste_on_blank(\n",
        "                prev_image_fix, interpol_width2)\n",
        "            interpol_image.paste(prev_image_fix_crop, mask=prev_image_fix_crop)\n",
        "\n",
        "            all_frames.append(interpol_image)\n",
        "        all_frames.append(current_image)\n",
        "        interpol_image.show()\n",
        "    video_file_name = \"infinite_zoom_\" + str(time.time())\n",
        "    fps = 30\n",
        "    save_path = video_file_name + \".mp4\"\n",
        "    start_frame_dupe_amount = 15\n",
        "    last_frame_dupe_amount = 15\n",
        "\n",
        "    write_video(save_path, all_frames, fps, False,\n",
        "                start_frame_dupe_amount, last_frame_dupe_amount)\n",
        "    return save_path\n",
        "\n",
        "\n",
        "def zoom_app():\n",
        "    with gr.Blocks():\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "\n",
        "                outpaint_prompts = gr.Dataframe(\n",
        "                    type=\"array\",\n",
        "                    headers=[\"outpaint steps\", \"prompt\"],\n",
        "                    datatype=[\"number\", \"str\"],\n",
        "                    row_count=1,\n",
        "                    col_count=(2, \"fixed\"),\n",
        "                    value=[[0, default_prompt]],\n",
        "                    wrap=True\n",
        "                )\n",
        "\n",
        "                outpaint_negative_prompt = gr.Textbox(\n",
        "                    lines=1,\n",
        "                    value=default_negative_prompt,\n",
        "                    label='Negative Prompt'\n",
        "                )\n",
        "\n",
        "                outpaint_steps = gr.Slider(\n",
        "                    minimum=5,\n",
        "                    maximum=25,\n",
        "                    step=1,\n",
        "                    value=12,\n",
        "                    label='Total Outpaint Steps'\n",
        "                )\n",
        "                with gr.Accordion(\"Advanced Options\", open=False):\n",
        "                    model_id = gr.Dropdown(\n",
        "                        choices=inpaint_model_list,\n",
        "                        value=inpaint_model_list[0],\n",
        "                        label='Pre-trained Model ID'\n",
        "                    )\n",
        "\n",
        "                    guidance_scale = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=15,\n",
        "                        step=0.1,\n",
        "                        value=7,\n",
        "                        label='Guidance Scale'\n",
        "                    )\n",
        "\n",
        "                    sampling_step = gr.Slider(\n",
        "                        minimum=1,\n",
        "                        maximum=100,\n",
        "                        step=1,\n",
        "                        value=50,\n",
        "                        label='Sampling Steps for each outpaint'\n",
        "                    )\n",
        "                    init_image = gr.Image(type=\"pil\",label=\"custom initial image\")\n",
        "                generate_btn = gr.Button(value='Generate video')\n",
        "\n",
        "            with gr.Column():\n",
        "                output_image = gr.Video(label='Output', format=\"mp4\").style(\n",
        "                    width=512, height=512)\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=zoom,\n",
        "            inputs=[\n",
        "                model_id,\n",
        "                outpaint_prompts,\n",
        "                outpaint_negative_prompt,\n",
        "                outpaint_steps,\n",
        "                guidance_scale,\n",
        "                sampling_step,\n",
        "                init_image\n",
        "            ],\n",
        "            outputs=output_image,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vhb-wxESGFGc",
        "outputId": "484ddc27-022d-4383-c9bf-eb04662f36a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Video' object has no attribute 'style'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-548733824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mzoom_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menable_queue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-4065960421.py\u001b[0m in \u001b[0;36mzoom_app\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 output_image = gr.Video(label='Output', format=\"mp4\").style(\n\u001b[0m\u001b[1;32m    188\u001b[0m                     width=512, height=512)\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Video' object has no attribute 'style'"
          ]
        }
      ],
      "source": [
        "#@title Launch App { display-mode: \"form\" }\n",
        "import gradio as gr\n",
        "\n",
        "app = gr.Blocks()\n",
        "with app:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <h2 style='text-align: center'>\n",
        "          <a href=\"https://github.com/v8hid/infinite-zoom-stable-diffusion/\" style=\"display:inline-block;\">\n",
        "            <img src=\"https://img.shields.io/static/v1?label=github&message=repository&color=blue&style=for-the-badge&logo=github&logoColor=white\" alt=\"build status\">\n",
        "          </a>\n",
        "          <br>\n",
        "        Text to Video - Infinite zoom effect\n",
        "        </h2>\n",
        "        \"\"\"\n",
        "    )\n",
        "    zoom_app()\n",
        "\n",
        "app.launch(share=True,debug=True,enable_queue=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}